{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gather/Scatter/Broadcast PyTorch/XLA",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jysohn23/xla/blob/model-parallel-colab/Gather_Scatter_Broadcast_PyTorch_XLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSMwPAIb5zI",
        "colab_type": "text"
      },
      "source": [
        "### Installing PyTorch/XLA\n",
        "\n",
        "Run the following cell (or copy it into your own notebook!) to install PyTorch, Torchvision, and PyTorch/XLA. It will take a couple minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20zLDvraY2mk",
        "colab_type": "code",
        "outputId": "2f8c86d6-203e-486c-e477-b44352215fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3727  100  3727    0     0  14334      0 --:--:-- --:--:-- --:--:-- 14389\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Uninstalling torch-1.4.0:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.4.0\n",
            "Uninstalling torchvision-0.5.0:\n",
            "  Successfully uninstalled torchvision-0.5.0\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][ 86.7 MiB/ 86.7 MiB]                                                \n",
            "Operation completed over 1 objects/86.7 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][116.0 MiB/116.0 MiB]                                                \n",
            "Operation completed over 1 objects/116.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.18.2)\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+aeb13f2\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+44429d0\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.6.0a0+aeb13f2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c2c002\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (378 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 133872 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X8podGsyQ9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "e1879f0e-b6ef-4bcf-9cea-b3dec27c3b0a"
      },
      "source": [
        "# all_gather with sub-groups via all_reduce\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "groups = [\n",
        "  [0, 1],\n",
        "  [2, 3],\n",
        "  [4, 5],\n",
        "  [6, 7],\n",
        "]\n",
        "group_size = len(groups[0])\n",
        "\n",
        "def all_gather(t: torch.Tensor) -> torch.Tensor:\n",
        "  s = torch.zeros((group_size,) + t.shape, dtype=torch.float32)\n",
        "  s[xm.get_ordinal() % group_size] = t\n",
        "  s = s.to(xm.xla_device())\n",
        "  xm.all_reduce('sum', [s], groups=groups)\n",
        "  return s\n",
        "\n",
        "def _mp_fn(rank: int):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  d = xm.xla_device()\n",
        "  t = torch.Tensor([xm.get_ordinal()] * 4)\n",
        "\n",
        "  print(f'ordinal={xm.get_ordinal()}, t={t}')\n",
        "  t_r = all_gather(t)\n",
        "  print(f'ordinal={xm.get_ordinal()}, all-reduced t_r={t_r}\\n')\n",
        "\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "xmp.spawn(_mp_fn, nprocs=8, start_method='fork')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ordinal=0, t=tensor([0., 0., 0., 0.])\n",
            "ordinal=4, t=tensor([4., 4., 4., 4.])\n",
            "ordinal=1, t=tensor([1., 1., 1., 1.])\n",
            "ordinal=2, t=tensor([2., 2., 2., 2.])\n",
            "ordinal=6, t=tensor([6., 6., 6., 6.])\n",
            "ordinal=5, t=tensor([5., 5., 5., 5.])\n",
            "ordinal=3, t=tensor([3., 3., 3., 3.])\n",
            "ordinal=7, t=tensor([7., 7., 7., 7.])\n",
            "ordinal=2, all-reduced t_r=tensor([[2., 2., 2., 2.],\n",
            "        [3., 3., 3., 3.]], device='xla:0')\n",
            "ordinal=4, all-reduced t_r=tensor([[4., 4., 4., 4.],\n",
            "        [5., 5., 5., 5.]], device='xla:0')\n",
            "ordinal=7, all-reduced t_r=tensor([[6., 6., 6., 6.],\n",
            "        [7., 7., 7., 7.]], device='xla:0')\n",
            "ordinal=0, all-reduced t_r=tensor([[0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.]], device='xla:1')\n",
            "ordinal=6, all-reduced t_r=tensor([[6., 6., 6., 6.],\n",
            "        [7., 7., 7., 7.]], device='xla:0')\n",
            "ordinal=3, all-reduced t_r=tensor([[2., 2., 2., 2.],\n",
            "        [3., 3., 3., 3.]], device='xla:0')\n",
            "ordinal=5, all-reduced t_r=tensor([[4., 4., 4., 4.],\n",
            "        [5., 5., 5., 5.]], device='xla:0')\n",
            "ordinal=1, all-reduced t_r=tensor([[0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.]], device='xla:0')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBqur3czyd_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "12c98329-a025-42a6-985b-43ed4eb79d9c"
      },
      "source": [
        "# Broadcast\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "groups = [\n",
        "  [0, 1, 2, 3],\n",
        "  [4, 5, 6, 7],\n",
        "]\n",
        "group_size = len(groups[0])\n",
        "size = 4\n",
        "\n",
        "# Note: with xla::CollectivePermute it should be also possible\n",
        "# to implement with something like (per group):\n",
        "#\n",
        "#   source_target_pairs = [[0,0], [0,1], [0,2], [0,3]]\n",
        "#\n",
        "\n",
        "def _broadcast_fn(rank: int):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  group_num = int(xm.get_ordinal() / group_size)\n",
        "  nsize = size * (group_num + 1)\n",
        "  t_cpu_0 = torch.linspace(start=0, end=nsize-1, steps=nsize) # broadcaster\n",
        "  t_cpu_i = torch.zeros(nsize) # receiver\n",
        "\n",
        "  # t_cpu for each ordinal:\n",
        "  # 0: tensor([0., 1., 2., 3.])\n",
        "  # 1: tensor([0., 0., 0., 0.])\n",
        "  # 2: tensor([0., 0., 0., 0.])\n",
        "  # 3: tensor([0., 0., 0., 0.])\n",
        "  # 4: tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
        "  # 5: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "  # 6: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "  # 7: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "  t_cpu = t_cpu_0 if xm.get_ordinal() % group_size == 0 else t_cpu_i\n",
        "  t = t_cpu.to(xm.xla_device())\n",
        "\n",
        "  print(f'ordinal={xm.get_ordinal()}, t={t}')\n",
        "  xm.all_reduce('sum', [t], groups=groups)\n",
        "  print(f'ordinal={xm.get_ordinal()}, all-reduced t={t}')\n",
        "\n",
        "  if t_cpu_0.tolist() != t.cpu().tolist():\n",
        "    print(f'Wrong result from core {xm.get_ordinal()}: {t}')\n",
        "  else:\n",
        "    print('ok')\n",
        "\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "xmp.spawn(_broadcast_fn, nprocs=8, start_method='fork')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ordinal=0, t=tensor([0., 1., 2., 3.], device='xla:1')\n",
            "ordinal=5, t=tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=1, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=3, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=7, t=tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=4, t=tensor([0., 1., 2., 3., 4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=2, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=6, t=tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=0, all-reduced t=tensor([0., 1., 2., 3.], device='xla:1')\n",
            "ordinal=4, all-reduced t=tensor([0., 1., 2., 3., 4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=7, all-reduced t=tensor([0., 1., 2., 3., 4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=5, all-reduced t=tensor([0., 1., 2., 3., 4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=6, all-reduced t=tensor([0., 1., 2., 3., 4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=3, all-reduced t=tensor([0., 1., 2., 3.], device='xla:0')\n",
            "ordinal=2, all-reduced t=tensor([0., 1., 2., 3.], device='xla:0')\n",
            "ordinal=1, all-reduced t=tensor([0., 1., 2., 3.], device='xla:0')\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ikp306uJRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "847a378d-3f63-47db-c972-a0743b5811ba"
      },
      "source": [
        "# Scatter (broadcast + _split)\n",
        "\n",
        "groups = [\n",
        "  [0, 1, 2, 3],\n",
        "  [4, 5, 6, 7],\n",
        "]\n",
        "group_size = len(groups[0])\n",
        "\n",
        "\n",
        "def _scatter_fn(rank):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  group_num = int(xm.get_ordinal() / group_size)\n",
        "  t_cpu_0 = torch.linspace(\n",
        "      start=group_num*group_size, end=(group_num+1)*group_size-1, steps=group_size) # broadcaster\n",
        "  t_cpu_i = torch.zeros(group_size) # receiver\n",
        "  t = t_cpu_0 if xm.get_ordinal() % group_size == 0 else t_cpu_i\n",
        "  t = t.to(xm.xla_device())\n",
        "\n",
        "  print(f'ordinal={xm.get_ordinal()}, t={t}')\n",
        "  xm.all_reduce('sum', [t], groups=groups) # bcast\n",
        "  t = t[xm.get_ordinal() % group_size] # _split\n",
        "  print(f'ordinal={xm.get_ordinal()}, bcasted t={t}\\n')\n",
        "\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "xmp.spawn(_scatter_fn, nprocs=8, start_method='fork')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ordinal=0, t=tensor([0., 1., 2., 3.], device='xla:1')\n",
            "ordinal=6, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=7, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=5, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=1, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=3, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=4, t=tensor([4., 5., 6., 7.], device='xla:0')\n",
            "ordinal=2, t=tensor([0., 0., 0., 0.], device='xla:0')\n",
            "ordinal=0, bcasted t=0.0\n",
            "ordinal=5, bcasted t=5.0\n",
            "ordinal=1, bcasted t=1.0\n",
            "ordinal=6, bcasted t=6.0\n",
            "ordinal=2, bcasted t=2.0\n",
            "ordinal=4, bcasted t=4.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ordinal=3, bcasted t=3.0\n",
            "ordinal=7, bcasted t=7.0\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rdVT7dpyh3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [WIP - AllToAll API pending] all_gather with sub-groups via all_to_all\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "groups = [\n",
        "  [0, 1],\n",
        "  [2, 3],\n",
        "  [4, 5],\n",
        "  [6, 7],\n",
        "]\n",
        "group_size = len(groups[0])\n",
        "\n",
        "def all_gather(t: torch.Tensor) -> torch.Tensor:\n",
        "  t = t.to(xm.xla_device())\n",
        "  s = xm.all_to_all(\n",
        "      t,\n",
        "      split_dimension=0,\n",
        "      concat_dimension=0,\n",
        "      split_count=4,\n",
        "      groups=groups)\n",
        "  return s\n",
        "\n",
        "def _mp_fn(rank: int):\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  d = xm.xla_device()\n",
        "  t = torch.Tensor([xm.get_ordinal()] * 4)\n",
        "\n",
        "  print(f'ordinal={xm.get_ordinal()}, t={t}')\n",
        "  t_r = all_gather(t)\n",
        "  print(f'ordinal={xm.get_ordinal()}, all-reduced t_r={t_r}\\n')\n",
        "\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "xmp.spawn(_mp_fn, nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}