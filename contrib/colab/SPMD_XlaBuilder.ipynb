{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPMD XlaBuilder",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmgM-6AGSY0Y",
        "outputId": "fd146c13-3ded-48de-ea90-ac700bd81b5f"
      },
      "source": [
        "import os\n",
        "os.environ['XLA_HLO_DEBUG'] = '1'\n",
        "if not os.environ.get('XRT_TPU_CONFIG'):\n",
        "  os.environ['XRT_TPU_CONFIG'] = 'localservice;0;localhost:51011'\n",
        "\n",
        "os.environ.get('XRT_TPU_CONFIG')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'localservice;0;localhost:51011'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhBpw2fhjX0p"
      },
      "source": [
        "## XlaBuilder Playground\n",
        "* [x] Simple Add operation with fwd/bwd to get familiar with `torch_xla.core.xla_builder` primitives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH2fWefxRBHf"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_builder as xb\n",
        "import torch_xla.core.xla_op_registry as xor\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNLT_4ycZYll"
      },
      "source": [
        "def _add_forward(x, y):\n",
        "  # type(x) = torch_xla.core.xla_builder.Op\n",
        "  # type(x.op) = _XLAC.XlaOp = op_builder::Op\n",
        "  builder = torch_xla._XLAC._xla_op_builder(x.op)\n",
        "  return xb.Op(torch_xla._XLAC._xla_op_create(builder, 'Add', [x.op, y.op], {}))\n",
        "\n",
        "ADD_FORWARD = xor.register('AddForward', _add_forward)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ4Y4GzqdQAR"
      },
      "source": [
        "class Add(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, y):\n",
        "    ctx.shape = x.shape\n",
        "    ctx.device = x.device\n",
        "    return ADD_FORWARD(x, y)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    grad = torch.ones(ctx.shape, device=ctx.device)\n",
        "    return grad, grad\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuEU_tdTPQk",
        "outputId": "7a867877-0d6e-44c3-d445-96d121268c52"
      },
      "source": [
        "device = xm.xla_device()\n",
        "x = torch.ones(1, 1, device=device, requires_grad=True)\n",
        "y = torch.randn(1, 1, device=device, requires_grad=True)\n",
        "\n",
        "output = Add.apply(x, y)\n",
        "# output = ADD_FORWARD(x, y)\n",
        "loss = output.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f'x: {x}')\n",
        "print(f'x.grad: {x.grad}')\n",
        "print(f'y: {y}')\n",
        "print(f'y.grad: {y.grad}')\n",
        "print(f'output: {output}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([[1.]], device='xla:1', requires_grad=True)\n",
            "x.grad: tensor([[1.]], device='xla:1')\n",
            "y: tensor([[0.1028]], device='xla:1', requires_grad=True)\n",
            "y.grad: tensor([[1.]], device='xla:1')\n",
            "output: tensor([[1.1028]], device='xla:1', grad_fn=<AddBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNtQtUBsdjZ1",
        "outputId": "9bd95b2c-c01d-4517-8329-f5b9af2fc45e"
      },
      "source": [
        "# Comparing result against native autograd\n",
        "t1 = torch.rand((1,1), requires_grad=True)\n",
        "t2 = torch.rand((1,1), requires_grad=True)\n",
        "\n",
        "s = t1 + t2\n",
        "s.sum().backward()\n",
        "\n",
        "print(t1)\n",
        "print(t1.grad)\n",
        "print(t2)\n",
        "print(t2.grad)\n",
        "print(s)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5333]], requires_grad=True)\n",
            "tensor([[1.]])\n",
            "tensor([[0.6611]], requires_grad=True)\n",
            "tensor([[1.]])\n",
            "tensor([[1.1944]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frOTKuek1CR-"
      },
      "source": [
        "## PyTorch / XLA for tracing, lowering -> JAX PjRt Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzKGIDf0_znO"
      },
      "source": [
        "import jax\n",
        "from jax.lib import xla_client as xc\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeaweF4Q1QUu",
        "outputId": "9b2b458f-d1aa-4193-e248-34a56c65d1af"
      },
      "source": [
        "two = (2 * torch.ones((1,1))).to(device)\n",
        "three = (3 * torch.ones((1,1))).to(device)\n",
        "six = two * three\n",
        "\n",
        "print(torch_xla._XLAC._get_xla_tensors_hlo([six]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HloModule IrToHlo.11\n",
            "\n",
            "ENTRY %IrToHlo.11 (p0.1: f32[1,1], p1.5: f32[1,1]) -> (f32[1,1]) {\n",
            "  %constant.2 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.3 = f32[1,1]{1,0} reshape(f32[] %constant.2), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.4 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.3), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %constant.6 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.7 = f32[1,1]{1,0} reshape(f32[] %constant.6), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.8 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.7), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %p1.5 = f32[1,1]{1,0} parameter(1), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %p0.1 = f32[1,1]{1,0} parameter(0), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %multiply.9 = f32[1,1]{1,0} multiply(f32[1,1]{1,0} %p1.5, f32[1,1]{1,0} %p0.1), metadata={op_type=\"aten__mul\" op_name=\"aten__mul\"}\n",
            "  ROOT %tuple.10 = (f32[1,1]{1,0}) tuple(f32[1,1]{1,0} %multiply.9)\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqz7-VDXB7L_",
        "outputId": "1aad2bfb-8c17-42ff-f4c3-b02f6877e972"
      },
      "source": [
        "backend = xc.get_local_backend()\n",
        "\n",
        "hlo_text = torch_xla._XLAC._get_xla_tensors_hlo([six])\n",
        "# JAX XlaComputation\n",
        "computation = xc.XlaComputation(\n",
        "    torch_xla._XLAC._hlo_text_to_serialized_xla_computation(hlo_text))\n",
        "# print(hlo_text)\n",
        "print(computation.as_hlo_text())\n",
        "\n",
        "compiled_computation = backend.compile(computation)\n",
        "compiled_computation.local_devices()\n",
        "\n",
        "# host_input = [t1.detach.numpy()]\n",
        "# # place host variable on device and execute\n",
        "# # device_input = backend.buffer_from_pyval(host_input)\n",
        "# compiled_computation.execute([host_input, ])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HloModule IrToHlo.11\n",
            "\n",
            "ENTRY IrToHlo.11 {\n",
            "  constant.2 = f32[] constant(0)\n",
            "  reshape.3 = f32[1,1]{1,0} reshape(constant.2)\n",
            "  broadcast.4 = f32[1,1]{1,0} broadcast(reshape.3), dimensions={0,1}\n",
            "  constant.6 = f32[] constant(0)\n",
            "  reshape.7 = f32[1,1]{1,0} reshape(constant.6)\n",
            "  broadcast.8 = f32[1,1]{1,0} broadcast(reshape.7), dimensions={0,1}\n",
            "  p1.5 = f32[1,1]{1,0} parameter(1)\n",
            "  p0.1 = f32[1,1]{1,0} parameter(0)\n",
            "  multiply.9 = f32[1,1]{1,0} multiply(p1.5, p0.1)\n",
            "  ROOT tuple.10 = (f32[1,1]{1,0}) tuple(multiply.9)\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUUA_6oVw6Cn",
        "outputId": "274a7fdf-9cff-4fc3-c57f-ac971f9b3d41"
      },
      "source": [
        "host_x = np.array([[3.0]], dtype=np.float32)\n",
        "device_x = backend.buffer_from_pyval(host_x)\n",
        "host_y = np.array([[4.0]], dtype=np.float32)\n",
        "device_y = backend.buffer_from_pyval(host_y)\n",
        "\n",
        "compiled_computation.execute([device_x, device_y])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DeviceArray([[12.]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS4loVkgjieR"
      },
      "source": [
        "## SPMD Playground\n",
        "Changes needed:\n",
        "* [x] XLA:HLO - Need to call [XlaBuilder::SetSharding](https://github.com/tensorflow/tensorflow/blob/7a18c91de6272c93468f9987d02a480a61a4b38c/tensorflow/compiler/xla/client/xla_builder.h#L195) with sharding annotation.\n",
        "* [x] Runtime - Once we have HLO dump generated by PyTorch / XLA, we'll leverage JAX runtime interface to compile and execute the SPMD HLO computation, given that we plan on migrating over to PjRt runtime, which has many bits currently missing in XRT that are needed for SPMD.\n",
        "  * [x] Load XLA HLO text as a `xla::XlaComputation`\n",
        "\n",
        "We use JAX to do the following:\n",
        "  * [x] Build options: (1) Set proper device assignment, (2) UseSpmdPartitioning set on compilation options.\n",
        "  * [x] Execution run on all cores.\n",
        "\n",
        "Testing:\n",
        "* [x] HLO Graph dump validation on sharding annotations.\n",
        "* [x] Dump from post SPMD partitioning pass (collectives insertion).\n",
        "* [x] Execution result validation.\n",
        "* [x] Multi-core concurrent execution full-traces sanity check.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twh5WqNU76Ic"
      },
      "source": [
        "## Sharded PyTorch / XLA HLO -> PjRt Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7s9VzJhzr5Q"
      },
      "source": [
        "from jax.experimental import sharded_jit\n",
        "from jax.experimental import PartitionSpec as P\n",
        "from jax._src.util import prod\n",
        "from jax.lib import xla_bridge as jxb"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiD56TbJM8I"
      },
      "source": [
        "s = jax.profiler.start_server(9012)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDLc7PD2CplH",
        "outputId": "91a00d97-4a10-4733-be90-9adee9b7aa5f"
      },
      "source": [
        "# Sharded tensor matmul\n",
        "shape = (2,2)\n",
        "p = P(*shape)\n",
        "py_opsharding = jxb._sharding_to_proto(p)\n",
        "\n",
        "\n",
        "def _sharded_mm(x, y):\n",
        "  builder = torch_xla._XLAC._xla_op_builder(x.op)\n",
        "\n",
        "  # Set sharding on PT / XLA Builder.\n",
        "  torch_xla._XLAC._xla_builder_set_sharding(\n",
        "      builder,\n",
        "      py_opsharding.replicate_on_last_tile_dim,\n",
        "      py_opsharding.tile_assignment_devices,\n",
        "      py_opsharding.tile_assignment_dimensions,\n",
        "      # py_opsharding.tuple_shardings,\n",
        "      int(py_opsharding.type))\n",
        "\n",
        "  # Make 'Sharding' XLA custom call.\n",
        "  x_sharded = x.custom_call('Sharding')\n",
        "  y_sharded = y.custom_call('Sharding')\n",
        "  return x_sharded @ y_sharded\n",
        "\n",
        "\n",
        "SHARDED_MM = xor.register('ShardedMatMul', _sharded_mm)\n",
        "\n",
        "\n",
        "# Dump HLO for execution on JAX PjRt.\n",
        "two = (2 * torch.ones((shape[0]*4096, shape[1]*4096))).to(device)\n",
        "three = (3 * torch.ones((shape[0]*4096, shape[1]*4096))).to(device)\n",
        "\n",
        "res = SHARDED_MM(two, three)\n",
        "sharded_hlo_text = torch_xla._XLAC._get_xla_tensors_hlo([res])\n",
        "print(sharded_hlo_text)\n",
        "\n",
        "\n",
        "# Execute sharded HLO on PjRt.\n",
        "computation = xc.XlaComputation(\n",
        "    torch_xla._XLAC._hlo_text_to_serialized_xla_computation(sharded_hlo_text))\n",
        "\n",
        "nrep = 1\n",
        "nparts = sum(shape) \n",
        "devices = jxb.local_devices()[:nparts]\n",
        "device_assignment = np.array([[d.id for d in devices]])\n",
        "device_assignment = np.reshape(device_assignment, (-1, nparts))\n",
        "\n",
        "compiled_computation = backend.compile(\n",
        "    computation,\n",
        "    jxb.get_compile_options(nrep, nparts, device_assignment))\n",
        "\n",
        "\n",
        "# Allocate device data.\n",
        "host_x = 3 * np.ones((4096,4096), dtype=np.float32)\n",
        "host_y = 4 * np.ones((4096,4096), dtype=np.float32)\n",
        "devices_x = [backend.buffer_from_pyval(host_x, device=device) for device in devices]\n",
        "devices_y = [backend.buffer_from_pyval(host_y, device=device) for device in devices]\n",
        "\n",
        "for step in range(3000):\n",
        "  if step % 100 == 0:\n",
        "    print(step)\n",
        "  with jax.profiler.StepTraceAnnotation(\"step\", step_num=step):\n",
        "    output = compiled_computation.execute_sharded_on_local_devices([devices_x, devices_y])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HloModule IrToHlo.21\n",
            "\n",
            "%ShardedMatMul.13 (p0.14: f32[8192,8192], p1.15: f32[8192,8192]) -> f32[8192,8192] {\n",
            "  %p0.14 = f32[8192,8192]{1,0} parameter(0)\n",
            "  %custom-call.16 = f32[8192,8192]{1,0} custom-call(f32[8192,8192]{1,0} %p0.14), custom_call_target=\"Sharding\", sharding={devices=[2,2]0,1,2,3}\n",
            "  %p1.15 = f32[8192,8192]{1,0} parameter(1)\n",
            "  %custom-call.17 = f32[8192,8192]{1,0} custom-call(f32[8192,8192]{1,0} %p1.15), custom_call_target=\"Sharding\", sharding={devices=[2,2]0,1,2,3}\n",
            "  ROOT %dot.18 = f32[8192,8192]{1,0} dot(f32[8192,8192]{1,0} %custom-call.16, f32[8192,8192]{1,0} %custom-call.17), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={devices=[2,2]0,1,2,3}\n",
            "}\n",
            "\n",
            "ENTRY %IrToHlo.21 (p0.1: f32[8192,8192], p1.7: f32[8192,8192]) -> (f32[8192,8192]) {\n",
            "  %constant.2 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.3 = f32[1,1]{1,0} reshape(f32[] %constant.2), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.4 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.3), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.5 = f32[] reshape(f32[1,1]{1,0} %broadcast.4), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.6 = f32[8192,8192]{1,0} broadcast(f32[] %reshape.5), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %constant.8 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.9 = f32[1,1]{1,0} reshape(f32[] %constant.8), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.10 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.9), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.11 = f32[] reshape(f32[1,1]{1,0} %broadcast.10), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.12 = f32[8192,8192]{1,0} broadcast(f32[] %reshape.11), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %p1.7 = f32[8192,8192]{1,0} parameter(1), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %p0.1 = f32[8192,8192]{1,0} parameter(0), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %call.19 = f32[8192,8192]{1,0} call(f32[8192,8192]{1,0} %p1.7, f32[8192,8192]{1,0} %p0.1), to_apply=%ShardedMatMul.13, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
            "  ROOT %tuple.20 = (f32[8192,8192]{1,0}) tuple(f32[8192,8192]{1,0} %call.19)\n",
            "}\n",
            "\n",
            "\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsMhRTy2_l-Z",
        "outputId": "7437b92a-21d0-4c78-9035-7acd9cea07cd"
      },
      "source": [
        "p = P(2,4)\n",
        "py_opsharding = jxb._sharding_to_proto(p)\n",
        "\n",
        "\n",
        "def _sharded_mul(x, y):\n",
        "  builder = torch_xla._XLAC._xla_op_builder(x.op)\n",
        "\n",
        "  # Set sharding on PT / XLA Builder.\n",
        "  torch_xla._XLAC._xla_builder_set_sharding(\n",
        "      builder,\n",
        "      py_opsharding.replicate_on_last_tile_dim,\n",
        "      py_opsharding.tile_assignment_devices,\n",
        "      py_opsharding.tile_assignment_dimensions,\n",
        "      # py_opsharding.tuple_shardings,\n",
        "      int(py_opsharding.type))\n",
        "\n",
        "  # Make 'Sharding' XLA custom call.\n",
        "  x_sharded = x.custom_call('Sharding')\n",
        "  y_sharded = y.custom_call('Sharding')\n",
        "  return x_sharded * y_sharded\n",
        "\n",
        "SHARDED_MULTIPLY = xor.register('ShardedMultiply', _sharded_mul)\n",
        "\n",
        "\n",
        "# Dump HLO for execution on JAX PjRt.\n",
        "two = (2 * torch.ones((2*4096,4*4096))).to(device)\n",
        "three = (3 * torch.ones((2*4096,4*4096))).to(device)\n",
        "\n",
        "six = SHARDED_MULTIPLY(two, three)\n",
        "sharded_hlo_text = torch_xla._XLAC._get_xla_tensors_hlo([six])\n",
        "print(sharded_hlo_text)\n",
        "\n",
        "\n",
        "# Execute sharded HLO on PjRt.\n",
        "computation = xc.XlaComputation(\n",
        "    torch_xla._XLAC._hlo_text_to_serialized_xla_computation(sharded_hlo_text))\n",
        "\n",
        "nrep = 1\n",
        "nparts = 8\n",
        "devices = jxb.local_devices()[:nparts]\n",
        "device_assignment = np.array([[d.id for d in devices]])\n",
        "device_assignment = np.reshape(device_assignment, (-1, nparts))\n",
        "\n",
        "compiled_computation = backend.compile(\n",
        "    computation,\n",
        "    jxb.get_compile_options(nrep, nparts, device_assignment))\n",
        "\n",
        "\n",
        "# Allocate device data.\n",
        "host_x = 3 * np.ones((4096,4096), dtype=np.float32)\n",
        "host_y = 4 * np.ones((4096,4096), dtype=np.float32)\n",
        "devices_x = [backend.buffer_from_pyval(host_x, device=device) for device in devices]\n",
        "devices_y = [backend.buffer_from_pyval(host_y, device=device) for device in devices]\n",
        "\n",
        "for step in range(300):\n",
        "  if step % 100 == 0:\n",
        "    print(step)\n",
        "  with jax.profiler.StepTraceAnnotation(\"step\", step_num=step):\n",
        "    output = compiled_computation.execute_sharded_on_local_devices([devices_x, devices_y])\n",
        "# print(output)\n",
        "# [(res.shape, res.device()) for res in output[0]]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HloModule IrToHlo.21\n",
            "\n",
            "%ShardedMultiply.13 (p0.14: f32[8192,16384], p1.15: f32[8192,16384]) -> f32[8192,16384] {\n",
            "  %p0.14 = f32[8192,16384]{1,0} parameter(0)\n",
            "  %custom-call.16 = f32[8192,16384]{1,0} custom-call(f32[8192,16384]{1,0} %p0.14), custom_call_target=\"Sharding\", sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n",
            "  %p1.15 = f32[8192,16384]{1,0} parameter(1)\n",
            "  %custom-call.17 = f32[8192,16384]{1,0} custom-call(f32[8192,16384]{1,0} %p1.15), custom_call_target=\"Sharding\", sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n",
            "  ROOT %multiply.18 = f32[8192,16384]{1,0} multiply(f32[8192,16384]{1,0} %custom-call.16, f32[8192,16384]{1,0} %custom-call.17), sharding={devices=[2,4]0,1,2,3,4,5,6,7}\n",
            "}\n",
            "\n",
            "ENTRY %IrToHlo.21 (p0.1: f32[8192,16384], p1.7: f32[8192,16384]) -> (f32[8192,16384]) {\n",
            "  %constant.2 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.3 = f32[1,1]{1,0} reshape(f32[] %constant.2), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.4 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.3), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.5 = f32[] reshape(f32[1,1]{1,0} %broadcast.4), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.6 = f32[8192,16384]{1,0} broadcast(f32[] %reshape.5), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %constant.8 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.9 = f32[1,1]{1,0} reshape(f32[] %constant.8), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.10 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.9), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.11 = f32[] reshape(f32[1,1]{1,0} %broadcast.10), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.12 = f32[8192,16384]{1,0} broadcast(f32[] %reshape.11), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %p1.7 = f32[8192,16384]{1,0} parameter(1), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %p0.1 = f32[8192,16384]{1,0} parameter(0), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %call.19 = f32[8192,16384]{1,0} call(f32[8192,16384]{1,0} %p1.7, f32[8192,16384]{1,0} %p0.1), to_apply=%ShardedMultiply.13, metadata={op_type=\"xla___op_ShardedMultiply\" op_name=\"xla___op_ShardedMultiply\"}\n",
            "  ROOT %tuple.20 = (f32[8192,16384]{1,0}) tuple(f32[8192,16384]{1,0} %call.19)\n",
            "}\n",
            "\n",
            "\n",
            "0\n",
            "100\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA6YC-PIZhZV"
      },
      "source": [
        "# # Manually writing HLO graph\n",
        "\n",
        "# sharded_hlo_text='''\n",
        "# HloModule IrToHlo.19\n",
        "\n",
        "# %AddForward.13 (p0.14: f32[2,2], p1.15: f32[2,2]) -> f32[2,2] {\n",
        "#   %p0.14 = f32[2,2]{1,0} parameter(0)\n",
        "#   %p1.15 = f32[2,2]{1,0} parameter(1)\n",
        "#   ROOT %add.16 = f32[2,2]{1,0} add(f32[2,2]{1,0} %p0.14, f32[2,2]{1,0} %p1.15)\n",
        "# }\n",
        "\n",
        "# ENTRY %IrToHlo.19 (p0.1: f32[2,2], p1.7: f32[2,2]) -> (f32[2,2]) {\n",
        "#   %constant.2 = f32[] constant(0)\n",
        "#   %reshape.3 = f32[1,1]{1,0} reshape(f32[] %constant.2)\n",
        "#   %broadcast.4 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.3), dimensions={0,1}\n",
        "#   %reshape.5 = f32[] reshape(f32[1,1]{1,0} %broadcast.4)\n",
        "#   %broadcast.6 = f32[2,2]{1,0} broadcast(f32[] %reshape.5), dimensions={}\n",
        "#   %constant.8 = f32[] constant(0)\n",
        "#   %reshape.9 = f32[1,1]{1,0} reshape(f32[] %constant.8)\n",
        "#   %broadcast.10 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.9), dimensions={0,1}\n",
        "#   %reshape.11 = f32[] reshape(f32[1,1]{1,0} %broadcast.10)\n",
        "#   %broadcast.12 = f32[2,2]{1,0} broadcast(f32[] %reshape.11), dimensions={}\n",
        "#   %p1.7 = f32[2,2]{1,0} parameter(1)\n",
        "#   %custom-call.1 = f32[2,2]{1,0} custom-call(%p1.7), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
        "#   %p0.1 = f32[2,2]{1,0} parameter(0)\n",
        "#   %custom-call.0 = f32[2,2]{1,0} custom-call(%p0.1), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
        "#   %call.17 = f32[2,2]{1,0} call(f32[2,2]{1,0} %custom-call.1, f32[2,2]{1,0} %custom-call.0), to_apply=%AddForward.13\n",
        "#   ROOT %tuple.18 = (f32[2,2]{1,0}) tuple(f32[2,2]{1,0} %call.17)\n",
        "# }\n",
        "# '''"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MGRHX6KI0rg",
        "outputId": "ef1142bc-3373-4330-c034-b64e23a037b6"
      },
      "source": [
        "p = P(2,1)\n",
        "py_opsharding = jxb._sharding_to_proto(p)\n",
        "\n",
        "\n",
        "def _sharded_add_forward_with_custom_call(x, y):\n",
        "  # type(x) = torch_xla.core.xla_builder.Op\n",
        "  # type(x.op) = _XLAC.XlaOp = op_builder::Op\n",
        "  builder = torch_xla._XLAC._xla_op_builder(x.op)\n",
        "\n",
        "  # Set sharding on PT / XLA Builder.\n",
        "  torch_xla._XLAC._xla_builder_set_sharding(\n",
        "      builder,\n",
        "      py_opsharding.replicate_on_last_tile_dim,\n",
        "      py_opsharding.tile_assignment_devices,\n",
        "      py_opsharding.tile_assignment_dimensions,\n",
        "      # py_opsharding.tuple_shardings,\n",
        "      int(py_opsharding.type))\n",
        "\n",
        "  # Make 'Sharding' XLA custom call.\n",
        "  x_sharded = x.custom_call('Sharding')\n",
        "  y_sharded = y.custom_call('Sharding')\n",
        "\n",
        "  # return xb.Op(torch_xla._XLAC._xla_op_create(\n",
        "  #     builder, 'Add', [x_sharded.op, y_sharded.op], {}))\n",
        "  return x_sharded + y_sharded\n",
        "\n",
        "\n",
        "SHARDED_ADD_FORWARD_WITH_CUSTOM_CALL = xor.register(\n",
        "    'ShardedAddForwardWithCustomCall', _sharded_add_forward_with_custom_call)\n",
        "\n",
        "\n",
        "# Dump HLO for execution on JAX PjRt.\n",
        "two = (2 * torch.ones((2,2))).to(device)\n",
        "three = (3 * torch.ones((2,2))).to(device)\n",
        "\n",
        "five = SHARDED_ADD_FORWARD_WITH_CUSTOM_CALL(two, three)\n",
        "sharded_hlo_text = torch_xla._XLAC._get_xla_tensors_hlo([five])\n",
        "print(sharded_hlo_text)\n",
        "\n",
        "\n",
        "# Execute the dumped HLO graph in JAX.\n",
        "computation = xc.XlaComputation(\n",
        "    torch_xla._XLAC._hlo_text_to_serialized_xla_computation(sharded_hlo_text))\n",
        "# print(computation.as_hlo_text())\n",
        "\n",
        "nrep = 1\n",
        "nparts = 2\n",
        "devices = jxb.local_devices()[:nparts]\n",
        "device_assignment = np.array([[d.id for d in devices]])\n",
        "device_assignment = np.reshape(device_assignment, (-1, nparts))\n",
        "\n",
        "compiled_computation = backend.compile(\n",
        "    computation,\n",
        "    jxb.get_compile_options(nrep, nparts, device_assignment))\n",
        "\n",
        "\n",
        "# Allocate device data.\n",
        "host_x = 3 * np.ones((1,2), dtype=np.float32)\n",
        "device_x_a = backend.buffer_from_pyval(host_x, device=devices[0])\n",
        "device_x_b = backend.buffer_from_pyval(host_x, device=devices[1])\n",
        "host_y = 4 * np.ones((1,2), dtype=np.float32)\n",
        "device_y_a = backend.buffer_from_pyval(host_y, device=devices[0])\n",
        "device_y_b = backend.buffer_from_pyval(host_y, device=devices[1])\n",
        "\n",
        "compiled_computation.execute_sharded_on_local_devices(\n",
        "    [[device_x_a, device_x_b], [device_y_a, device_y_b]])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HloModule IrToHlo.21\n",
            "\n",
            "%ShardedAddForwardWithCustomCall.13 (p0.14: f32[2,2], p1.15: f32[2,2]) -> f32[2,2] {\n",
            "  %p0.14 = f32[2,2]{1,0} parameter(0)\n",
            "  %custom-call.16 = f32[2,2]{1,0} custom-call(f32[2,2]{1,0} %p0.14), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
            "  %p1.15 = f32[2,2]{1,0} parameter(1)\n",
            "  %custom-call.17 = f32[2,2]{1,0} custom-call(f32[2,2]{1,0} %p1.15), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
            "  ROOT %add.18 = f32[2,2]{1,0} add(f32[2,2]{1,0} %custom-call.16, f32[2,2]{1,0} %custom-call.17), sharding={devices=[2,1]0,1}\n",
            "}\n",
            "\n",
            "ENTRY %IrToHlo.21 (p0.1: f32[2,2], p1.7: f32[2,2]) -> (f32[2,2]) {\n",
            "  %constant.2 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.3 = f32[1,1]{1,0} reshape(f32[] %constant.2), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.4 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.3), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.5 = f32[] reshape(f32[1,1]{1,0} %broadcast.4), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.6 = f32[2,2]{1,0} broadcast(f32[] %reshape.5), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %constant.8 = f32[] constant(0), metadata={op_type=\"prim__Constant\" op_name=\"prim__Constant\"}\n",
            "  %reshape.9 = f32[1,1]{1,0} reshape(f32[] %constant.8), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.10 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.9), dimensions={0,1}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %reshape.11 = f32[] reshape(f32[1,1]{1,0} %broadcast.10), metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %broadcast.12 = f32[2,2]{1,0} broadcast(f32[] %reshape.11), dimensions={}, metadata={op_type=\"aten__expand\" op_name=\"aten__expand\"}\n",
            "  %p1.7 = f32[2,2]{1,0} parameter(1), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %p0.1 = f32[2,2]{1,0} parameter(0), metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
            "  %call.19 = f32[2,2]{1,0} call(f32[2,2]{1,0} %p1.7, f32[2,2]{1,0} %p0.1), to_apply=%ShardedAddForwardWithCustomCall.13, metadata={op_type=\"xla___op_ShardedAddForwardWithCustomCall\" op_name=\"xla___op_ShardedAddForwardWithCustomCall\"}\n",
            "  ROOT %tuple.20 = (f32[2,2]{1,0}) tuple(f32[2,2]{1,0} %call.19)\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[DeviceArray([[7., 7.],\n",
              "               [7., 7.]], dtype=float32),\n",
              "  DeviceArray([[7., 7.],\n",
              "               [7., 7.]], dtype=float32)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RxEyAgV5d7r"
      },
      "source": [
        "#### Sample SPMD parititoner pass runtime logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc038Tmt5v1F"
      },
      "source": [
        "```\n",
        "I0615 14:52:19.273297 1044120 2a886c8_compiler_base.cc:3450] XLA::TPU running hlo passes for 19 instructions, modules: IrToHlo.21\n",
        "I0615 14:52:19.273737 1044120 2a886c8_compiler_base.cc:3517] HLO optimizing 9 instructions\n",
        "I0615 14:52:19.273757 1044120 2a886c8_compiler_base.cc:3532] XLA::TPU HLO optimization\n",
        "I0615 14:52:19.274559 1044120 spmd_partitioner.cc:3533]\n",
        "I0615 14:52:19.274578 1044120 spmd_partitioner.cc:3533]\n",
        "I0615 14:52:19.274580 1044120 spmd_partitioner.cc:3533] ***** SPMD memory usage before partition *****\n",
        "I0615 14:52:19.274582 1044120 spmd_partitioner.cc:3533]\n",
        "I0615 14:52:19.274584 1044120 spmd_partitioner.cc:3533]   ** Replicated instructions\n",
        "I0615 14:52:19.274585 1044120 spmd_partitioner.cc:3533]\n",
        "I0615 14:52:19.274587 1044120 spmd_partitioner.cc:3533]   ** All instructions\n",
        "I0615 14:52:19.274589 1044120 spmd_partitioner.cc:3533]   256.00MiB : %p1.7 = f32[8192,8192]{1,0} parameter(1), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
        "I0615 14:52:19.274591 1044120 spmd_partitioner.cc:3533]   256.00MiB : %p0.1 = f32[8192,8192]{1,0} parameter(0), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
        "I0615 14:52:19.274593 1044120 spmd_partitioner.cc:3533]   256.00MiB : %convolution = f32[8192,8192]{1,0} convolution(f32[8192,8192]{1,0} %p1.7, f32[8192,8192]{1,0} %p0.1), dim_labels=bf_io->bf, sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.274640 1044120 spmd_partitioner.cc:3242] Partitioning computation IrToHlo.21 for 1 replicas and 4 partitions\n",
        "I0615 14:52:19.275185 1044120 spmd_partitioner.cc:373] Resharding %param = f32[4096,4096]{1,0} parameter(1), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"} from {devices=[2,2]0,1,2,3} to {devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}\n",
        "I0615 14:52:19.275408 1044120 spmd_partitioner.cc:373] Resharding %param = f32[4096,4096]{1,0} parameter(0), sharding={devices=[2,1]0,1}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"} from {devices=[2,1]0,1} to {replicated}\n",
        "I0615 14:52:19.275618 1044120 spmd_partitioner.cc:373] Resharding %convolution = f32[4096,4096]{1,0} convolution(f32[4096,8192]{1,0} %reshape, f32[8192,4096]{1,0} %reshape), dim_labels=bf_io->bf, sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"} from {devices=[2,2]0,1,2,3} to {replicated}\n",
        "I0615 14:52:19.275768 1044120 spmd_partitioner.cc:3595]\n",
        "I0615 14:52:19.275778 1044120 spmd_partitioner.cc:3595]\n",
        "I0615 14:52:19.275780 1044120 spmd_partitioner.cc:3595] ***** SPMD memory usage after partition *****\n",
        "I0615 14:52:19.275781 1044120 spmd_partitioner.cc:3595]   256.00MiB : %all-gather.2 = f32[4,4096,4096]{2,1,0} all-gather(f32[1,4096,4096]{2,1,0} %reshape.10), channel_id=3, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true\n",
        "I0615 14:52:19.275787 1044120 spmd_partitioner.cc:3595]   256.00MiB : %reshape.12 = f32[2,2,4096,4096]{3,2,1,0} reshape(f32[4,4096,4096]{2,1,0} %all-gather.2)\n",
        "I0615 14:52:19.275788 1044120 spmd_partitioner.cc:3595]   256.00MiB : %transpose.2 = f32[2,4096,2,4096]{3,1,2,0} transpose(f32[2,2,4096,4096]{3,2,1,0} %reshape.12), dimensions={0,2,1,3}\n",
        "I0615 14:52:19.275790 1044120 spmd_partitioner.cc:3595]   256.00MiB : %reshape.13 = f32[8192,8192]{1,0} reshape(f32[2,4096,2,4096]{3,1,2,0} %transpose.2), sharding={replicated}\n",
        "I0615 14:52:19.275792 1044120 spmd_partitioner.cc:3595]   128.00MiB : %all-gather = f32[2,4096,4096]{2,1,0} all-gather(f32[1,4096,4096]{2,1,0} %reshape.4), channel_id=1, replica_groups={{0,1},{2,3}}, dimensions={0}, use_global_device_ids=true, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275802 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275806 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275807 1044120 spmd_partitioner.cc:3596] ***** SPMD memory during transformation *****\n",
        "I0615 14:52:19.275809 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275811 1044120 spmd_partitioner.cc:3596]   256.00MiB : %tuple = (f32[8192,8192]{1,0}) tuple(f32[8192,8192]{1,0} %reshape), sharding={{replicated}}     * %reshape = f32[1,4096,4096]{2,1,0} reshape(f32[4096,4096]{1,0} %convolution)\n",
        "I0615 14:52:19.275812 1044120 spmd_partitioner.cc:3596]      * %all-gather = f32[4,4096,4096]{2,1,0} all-gather(f32[1,4096,4096]{2,1,0} %reshape), channel_id=3, replica_groups={{0,1,2,3}}, dimensions={0}, use_global_device_ids=true\n",
        "I0615 14:52:19.275814 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[2,2,4096,4096]{3,2,1,0} reshape(f32[4,4096,4096]{2,1,0} %all-gather)\n",
        "I0615 14:52:19.275816 1044120 spmd_partitioner.cc:3596]      * %transpose = f32[2,4096,2,4096]{3,1,2,0} transpose(f32[2,2,4096,4096]{3,2,1,0} %reshape), dimensions={0,2,1,3}\n",
        "I0615 14:52:19.275817 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[8192,8192]{1,0} reshape(f32[2,4096,2,4096]{3,1,2,0} %transpose), sharding={replicated}\n",
        "I0615 14:52:19.275819 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275820 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275822 1044120 spmd_partitioner.cc:3596]   128.00MiB : %convolution = f32[4096,4096]{1,0} convolution(f32[4096,8192]{1,0} %reshape, f32[8192,4096]{1,0} %reshape), dim_labels=bf_io->bf, sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}     * %constant = u32[4]{0} constant({0, 0, 1, 1}), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275823 1044120 spmd_partitioner.cc:3596]      * %dynamic-slice = u32[1]{0} dynamic-slice(u32[4]{0} %constant, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275825 1044120 spmd_partitioner.cc:3596]      * %reshape = u32[] reshape(u32[1]{0} %dynamic-slice), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275827 1044120 spmd_partitioner.cc:3596]      * %constant = s32[4]{0} constant({0, 0, 1, 1}), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275828 1044120 spmd_partitioner.cc:3596]      * %dynamic-slice = s32[1]{0} dynamic-slice(s32[4]{0} %constant, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275830 1044120 spmd_partitioner.cc:3596]      * %reshape = s32[] reshape(s32[1]{0} %dynamic-slice), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275832 1044120 spmd_partitioner.cc:3596]      * %constant = s32[] constant(0), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275833 1044120 spmd_partitioner.cc:3596]      * %constant = u32[4]{0} constant({0, 1, 2, 3}), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275835 1044120 spmd_partitioner.cc:3596]      * %dynamic-slice = u32[1]{0} dynamic-slice(u32[4]{0} %constant, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275837 1044120 spmd_partitioner.cc:3596]      * %reshape = u32[] reshape(u32[1]{0} %dynamic-slice), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275839 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[1,4096,4096]{2,1,0} reshape(f32[4096,4096]{1,0} %param), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275840 1044120 spmd_partitioner.cc:3596]      * %all-gather = f32[2,4096,4096]{2,1,0} all-gather(f32[1,4096,4096]{2,1,0} %reshape), channel_id=1, replica_groups={{0,1},{2,3}}, dimensions={0}, use_global_device_ids=true, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275842 1044120 spmd_partitioner.cc:3596]      * %transpose = f32[4096,2,4096]{2,0,1} transpose(f32[2,4096,4096]{2,1,0} %all-gather), dimensions={1,0,2}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275843 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[4096,8192]{1,0} reshape(f32[4096,2,4096]{2,0,1} %transpose), sharding={devices=[2,1,2]0,1,2,3 last_tile_dim_replicate}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275845 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[1,4096,4096]{2,1,0} reshape(f32[4096,4096]{1,0} %param), metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275847 1044120 spmd_partitioner.cc:3596]      * %all-gather = f32[2,4096,4096]{2,1,0} all-gather(f32[1,4096,4096]{2,1,0} %reshape), channel_id=2, replica_groups={{0,2},{1,3}}, dimensions={0}, use_global_device_ids=true, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275848 1044120 spmd_partitioner.cc:3596]      * %transpose = f32[2,4096,4096]{2,1,0} transpose(f32[2,4096,4096]{2,1,0} %all-gather), dimensions={0,1,2}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275850 1044120 spmd_partitioner.cc:3596]      * %reshape = f32[8192,4096]{1,0} reshape(f32[2,4096,4096]{2,1,0} %transpose), sharding={replicated}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275851 1044120 spmd_partitioner.cc:3596]      * %convolution = f32[4096,4096]{1,0} convolution(f32[4096,8192]{1,0} %reshape, f32[8192,4096]{1,0} %reshape), dim_labels=bf_io->bf, sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla___op_ShardedMatMul\" op_name=\"xla___op_ShardedMatMul\"}\n",
        "I0615 14:52:19.275853 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275854 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275856 1044120 spmd_partitioner.cc:3596]   64.00MiB : %param = f32[4096,4096]{1,0} parameter(1), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}     * %param = f32[4096,4096]{1,0} parameter(1), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
        "I0615 14:52:19.275858 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275860 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.275862 1044120 spmd_partitioner.cc:3596]   64.00MiB : %param = f32[4096,4096]{1,0} parameter(0), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}     * %param = f32[4096,4096]{1,0} parameter(0), sharding={devices=[2,2]0,1,2,3}, metadata={op_type=\"xla__device_data\" op_name=\"xla__device_data\"}\n",
        "I0615 14:52:19.275863 1044120 spmd_partitioner.cc:3596]\n",
        "I0615 14:52:19.277370 1044120 tpu_layout_assignment.cc:2037] Ran 2 additional passes of layout assignment to assign all layouts.\n",
        "I0615 14:52:19.278897 1044120 window_config_assignment.cc:62] Retrieving backend configs from FDO profiles.\n",
        "I0615 14:52:19.278940 1044120 2a886c8_compiler_base.cc:3218] XLA::TPU HLO PostOptimizationPipeline\n",
        "I0615 14:52:20.832438 1044120 2a886c8_compiler_base.cc:1494] final program bundle count: 22,496 note this count does not reflect cycles spent executing delays.\n",
        "I0615 14:52:20.836553 1044120 2a886c8_compiler_base.cc:1494] final program bundle count: 60 note this count does not reflect cycles spent executing delays.\n",
        "I0615 14:52:20.869977 1044120 2a886c8_compiler_base.cc:1673] Program too large for IMEM. Divided into 2 overlays (946.0K).\n",
        "I0615 14:52:20.877122 1044120 2a886c8_compiler_base.cc:1799] XLA::TPU program HBM usage: 512.97M / 15.48G\n",
        "I0615 14:52:20.877145 1044120 2a886c8_compiler_base.cc:1829] XLA::TPU program VMEM usage: 15.00M / 16.00M\n",
        "I0615 14:52:20.877158 1044120 2a886c8_compiler_base.cc:1840] Total hbm usage >= 1.14G:\n",
        "I0615 14:52:20.877161 1044120 2a886c8_compiler_base.cc:1840]     reserved        530.00M\n",
        "I0615 14:52:20.877162 1044120 2a886c8_compiler_base.cc:1840]     program         512.97M\n",
        "I0615 14:52:20.877163 1044120 2a886c8_compiler_base.cc:1840]     arguments       128.00M\n",
        "I0615 14:52:20.877165 1044120 2a886c8_compiler_base.cc:1840]\n",
        "I0615 14:52:20.877167 1044120 2a886c8_compiler_base.cc:1840] Output size 256.00M; shares 0B with arguments.\n",
        "I0615 14:52:20.877168 1044120 2a886c8_compiler_base.cc:1840]\n",
        "I0615 14:52:20.877183 1044120 2a886c8_compiler_base.cc:1844] Program sflag requirement 128B:\n",
        "I0615 14:52:20.877185 1044120 2a886c8_compiler_base.cc:1844]     reserved           100B\n",
        "I0615 14:52:20.877187 1044120 2a886c8_compiler_base.cc:1844]     scoped              28B\n",
        "I0615 14:52:20.877188 1044120 2a886c8_compiler_base.cc:1844] Program hbm requirement 512.97M:\n",
        "I0615 14:52:20.877190 1044120 2a886c8_compiler_base.cc:1844]     global            52.0K\n",
        "I0615 14:52:20.877191 1044120 2a886c8_compiler_base.cc:1844]     HLO temp        512.00M (100.0% utilization: Unpadded (512.00M) Padded (512.00M), 0.0% fragmentation (0B))\n",
        "I0615 14:52:20.877193 1044120 2a886c8_compiler_base.cc:1844]     overlays         946.0K\n",
        "I0615 14:52:20.877195 1044120 2a886c8_compiler_base.cc:1844] Program vmem requirement 15.00M:\n",
        "I0615 14:52:20.877196 1044120 2a886c8_compiler_base.cc:1844]     scoped           15.00M\n",
        "I0615 14:52:20.877197 1044120 2a886c8_compiler_base.cc:1844] Program smem requirement 2.0K:\n",
        "I0615 14:52:20.877199 1044120 2a886c8_compiler_base.cc:1844]     scoped             2.0K\n",
        "I0615 14:52:20.877200 1044120 2a886c8_compiler_base.cc:1852] XLA::TPU program SMEM usage: 2.3K / 16.0K (2 parameters)\n",
        "I0615 14:52:20.880040 1044120 isa_program.cc:370] Executable fingerprint:875f932b296c245088309b0b2e4ada4df6bc2f54bdb249b50a9622cfbf651470\n",
        "I0615 14:52:29.849659 1044716 futex.cc:60] RAW: Futex::Swap(): using FUTEX_WAKE + FUTEX_WAIT\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf1dsYmm4eoo"
      },
      "source": [
        "## Sharded JIT graph HLO graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srWuchSUqqH-"
      },
      "source": [
        "from jax.experimental import sharded_jit\n",
        "from jax.experimental import PartitionSpec as P\n",
        "from jax._src.util import prod\n",
        "from jax.lib import xla_bridge as jxb"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WddovfnyqrQC",
        "outputId": "2075cdc8-a55f-4514-b377-64b4e4fd407e"
      },
      "source": [
        "p = P(2,1)\n",
        "py_opsharding = jxb._sharding_to_proto(p)\n",
        "py_opsharding\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<jaxlib.xla_client.OpSharding at 0x7f66c41eec20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28lTJG994jED",
        "outputId": "a07fe5c5-6099-4e1f-93e6-e08a496dfa43"
      },
      "source": [
        "def f(x, y):\n",
        "  return x * y\n",
        "\n",
        "sharded_f = sharded_jit(f, in_parts=(P(2, 1), P(2, 1)), out_parts=(P(2, 1)))\n",
        "\n",
        "shape = (2, 2)\n",
        "x = np.arange(prod(shape), dtype=np.float32).reshape(shape)\n",
        "y = np.arange(prod(shape), dtype=np.float32).reshape(shape)\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "z = sharded_f(x, y)\n",
        "print(z)\n",
        "jax_sharded_hlo = jax.xla_computation(sharded_f)(x, x).as_hlo_text()\n",
        "print(jax_sharded_hlo)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [2. 3.]]\n",
            "[[0. 1.]\n",
            " [2. 3.]]\n",
            "[[0. 1.]\n",
            " [4. 9.]]\n",
            "HloModule xla_computation_f.16\n",
            "\n",
            "sharded_jit_f.4 {\n",
            "  constant.9 = pred[] constant(false)\n",
            "  parameter.5 = f32[2,2]{1,0} parameter(0)\n",
            "  custom-call.6 = f32[2,2]{1,0} custom-call(parameter.5), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
            "  parameter.7 = f32[2,2]{1,0} parameter(1)\n",
            "  custom-call.8 = f32[2,2]{1,0} custom-call(parameter.7), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
            "  multiply.10 = f32[2,2]{1,0} multiply(custom-call.6, custom-call.8)\n",
            "  custom-call.11 = f32[2,2]{1,0} custom-call(multiply.10), custom_call_target=\"Sharding\", sharding={devices=[2,1]0,1}\n",
            "  ROOT tuple.12 = (f32[2,2]{1,0}) tuple(custom-call.11)\n",
            "}\n",
            "\n",
            "ENTRY xla_computation_f.16 {\n",
            "  constant.3 = pred[] constant(false)\n",
            "  parameter.1 = f32[2,2]{1,0} parameter(0)\n",
            "  parameter.2 = f32[2,2]{1,0} parameter(1)\n",
            "  call.13 = (f32[2,2]{1,0}) call(parameter.1, parameter.2), to_apply=sharded_jit_f.4\n",
            "  get-tuple-element.14 = f32[2,2]{1,0} get-tuple-element(call.13), index=0\n",
            "  ROOT tuple.15 = (f32[2,2]{1,0}) tuple(get-tuple-element.14)\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}